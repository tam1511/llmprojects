{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0547f5cd",
   "metadata": {},
   "source": [
    "# 1. start with small idea of RAG - search by keywords matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf147fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import functools\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5027b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI()\n",
    "\n",
    "# Configuration\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "821a99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance improvement: Use caching for context loading\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def load_context():\n",
    "    \"\"\"Load and cache context data to avoid repeated file I/O\"\"\"\n",
    "    context = {}\n",
    "    \n",
    "    # Load employees with threading for faster I/O\n",
    "    def load_employee_files():\n",
    "        employee_context = {}\n",
    "        employees = glob.glob(\"knowledge-base/employees/*\")\n",
    "        \n",
    "        def load_single_employee(employee):\n",
    "            name = employee.split(' ')[-1][:-3]\n",
    "            try:\n",
    "                with open(employee, \"r\", encoding=\"utf-8\") as f:\n",
    "                    return name, f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {employee}: {e}\")\n",
    "                return name, \"\"\n",
    "        \n",
    "        # Use ThreadPoolExecutor for concurrent file loading\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            results = executor.map(load_single_employee, employees)\n",
    "            employee_context.update(dict(results))\n",
    "        \n",
    "        return employee_context\n",
    "    \n",
    "    # Load schools with threading\n",
    "    def load_school_files():\n",
    "        school_context = {}\n",
    "        schools = glob.glob(\"knowledge-base/schools/*\")\n",
    "        \n",
    "        def load_single_school(school):\n",
    "            name = school.split(os.sep)[-1][:-3]\n",
    "            try:\n",
    "                with open(school, \"r\", encoding=\"utf-8\") as f:\n",
    "                    return name, f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {school}: {e}\")\n",
    "                return name, \"\"\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            results = executor.map(load_single_school, schools)\n",
    "            school_context.update(dict(results))\n",
    "        \n",
    "        return school_context\n",
    "    \n",
    "    # Load both types concurrently\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        employee_future = executor.submit(load_employee_files)\n",
    "        school_future = executor.submit(load_school_files)\n",
    "        \n",
    "        context.update(employee_future.result())\n",
    "        context.update(school_future.result())\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load context once at startup\n",
    "context = load_context()\n",
    "print(f\"Loaded context with {len(context)} documents: {list(context.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "context[\"Lan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87455ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = (\n",
    "    \"Bạn là một chuyên gia tư vấn du học Hàn Quốc tại trung tâm Korea Study. \"\n",
    "    \"Nhiệm vụ của bạn là trả lời các câu hỏi liên quan đến trung tâm, nhân viên, trường học, và thông tin visa một cách ngắn gọn và chính xác. \"\n",
    "    \"Nếu bạn không biết câu trả lời, hãy nói rõ rằng bạn không biết. \"\n",
    "    \"Tuyệt đối không bịa ra thông tin nếu không có ngữ cảnh liên quan được cung cấp.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(message):\n",
    "    relevant_context = []\n",
    "    for context_title, context_details in context.items():\n",
    "        if context_title.lower() in message.lower():\n",
    "            relevant_context.append(context_details)\n",
    "    return relevant_context  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc899a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevant_context(\"Who is Timi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1083ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevant_context(\"who is Nguyễn Thị Lan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevant_context(\"who is Nguyễn Thị Lan and introduce about Yonsei University\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevant_context(\"who is nguyen thi la and introduce about yonsei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541bdbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(message):\n",
    "    \"\"\"Add relevant context to message\"\"\"\n",
    "    relevant_context = get_relevant_context(message)\n",
    "    if relevant_context:\n",
    "        message += \"\\n\\nNhững thông tin sau có thể hữu ích cho việc trả lời câu hỏi này:\\n\\n\"\n",
    "        for relevant in relevant_context:\n",
    "            message += relevant + \"\\n\\n\"\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97bf6db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \"\"\"Optimized chat function with better error handling\"\"\"\n",
    "    try:\n",
    "        messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
    "        message = add_context(message)\n",
    "        messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=MODEL, \n",
    "            messages=messages, \n",
    "            stream=True,\n",
    "            max_tokens=1000,  # Limit response length for faster generation\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                response += chunk.choices[0].delta.content\n",
    "                yield response\n",
    "    except Exception as e:\n",
    "        yield f\"Xin lỗi, đã có lỗi xảy ra: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch first version\n",
    "print(\"Launching keyword-based RAG chatbot...\")\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b845f7",
   "metadata": {},
   "source": [
    "# 2. RAG bigger idea with vector search - optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6f86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993691d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "#text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# Nếu dòng trên không hoạt động, người dùng Windows có thể dùng dòng dưới thay thế\n",
    "text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)\n",
    "\n",
    "print(\"Total documents loaded:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbad489",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # Slightly smaller chunks for better retrieval\n",
    "    chunk_overlap=100,  # Reduced overlap for performance\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Better separation\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080bfdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5fe65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Các loại tài liệu đã tìm thấy: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    if 'Nguyễn Thị Lan' in chunk.page_content:\n",
    "        print(chunk)\n",
    "        print(\"_________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "536af010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fffc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đưa các đoạn văn bản (chunks) vào Vector Store, liên kết mỗi đoạn với một vector embedding\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Nếu bạn muốn sử dụng embeddings miễn phí từ HuggingFace (thay vì OpenAI),\n",
    "# hãy thay dòng embeddings = OpenAIEmbeddings()\n",
    "# bằng:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d2ae223",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Đặt tên cho database vector (có thể tùy chọn)\n",
    "db_name = \"vector_db\"\n",
    "\n",
    "# Kiểm tra nếu database Chroma đã tồn tại, thì xóa collection để khởi động lại từ đầu\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo vector store bằng Chroma\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,              # Danh sách các đoạn văn bản đã chia nhỏ\n",
    "    embedding=embeddings,          # Hàm embedding (ví dụ: OpenAI hoặc HuggingFace)\n",
    "    persist_directory=db_name      # Thư mục lưu trữ database\n",
    ")\n",
    "# Kiểm tra số lượng document đã được lưu vào vector store\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy ra bộ sưu tập vector từ vectorstore\n",
    "collection = vectorstore._collection\n",
    "\n",
    "# Lấy 1 embedding từ database\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "\n",
    "# Kiểm tra số chiều (số phần tử trong vector)\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda31482",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17e1f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy toàn bộ vector, tài liệu và metadata từ collection\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "\n",
    "# Đưa embedding vào mảng numpy\n",
    "vectors = np.array(result['embeddings'])\n",
    "\n",
    "# Lưu lại văn bản\n",
    "documents = result['documents']\n",
    "\n",
    "# Trích loại tài liệu từ metadata (giả sử có 'doc_type')\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "\n",
    "# Gán màu sắc tùy theo loại tài liệu\n",
    "colors = [['blue', 'green', 'red', 'orange'][['company', 'employees', 'visas', 'schools'].index(t)] for t in doc_types]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbf6fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con người chúng ta dễ hình dung mọi thứ trong không gian 2D hơn!\n",
    "# Giảm số chiều của vector xuống 2D bằng t-SNE\n",
    "# (T-distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Tạo biểu đồ scatter 2D\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Loại: {t}<br>Văn bản: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Biểu đồ 2D Chroma Vector Store',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71d34dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory  \n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo mô hình Chat với OpenAI\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7, \n",
    "    model_name=MODEL,\n",
    ")\n",
    "\n",
    "# Thiết lập bộ nhớ hội thoại\n",
    "memory = ConversationBufferWindowMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# Tạo retriever từ vector store (Chroma)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Kết nối tất cả thành một chuỗi hội thoại có khả năng truy xuất (RAG pipeline)\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e53e7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query with performance monitoring\n",
    "def test_query_performance():\n",
    "    \"\"\"Test query with timing\"\"\"\n",
    "    query = \"Bạn có thể mô tả ngắn gọn về Korea study center không?\"\n",
    "    start_time = time.time()\n",
    "    result = conversation_chain.invoke({\"question\": query})\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Query processed in {end_time - start_time:.2f} seconds\")\n",
    "    print(\"Answer:\", result[\"answer\"])\n",
    "    if \"source_documents\" in result:\n",
    "        print(f\"Used {len(result['source_documents'])} source documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dfbaeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferWindowMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa6eca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c9bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01bc5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hãy cùng tìm hiểu xem điều gì được gửi phía sau hậu trường\n",
    "\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever, \n",
    "    memory=memory, \n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53c9610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferWindowMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 30})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever, \n",
    "    memory=memory, \n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e971c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400f907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf90389e",
   "metadata": {},
   "source": [
    "# 3 Improve RAG Ensemble Hybrid Retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseRetriever, Document\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from typing import List, Dict\n",
    "\n",
    "# 1. Fix keyword retriever to search content, not just title\n",
    "class KeywordRetriever(BaseRetriever):\n",
    "    context_dict: Dict[str, str]\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        relevant_docs = []\n",
    "        for title, content in self.context_dict.items():\n",
    "            if any(kw in content.lower() for kw in query.lower().split()):\n",
    "                relevant_docs.append(Document(page_content=content, metadata={\"source\": title}))\n",
    "        return relevant_docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self.get_relevant_documents(query)\n",
    "\n",
    "# 2. Use keyword + properly configured vector retriever\n",
    "keyword_retriever = KeywordRetriever(context_dict=context)\n",
    "vector_retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 30}\n",
    ")\n",
    "\n",
    "# 3. Ensemble\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[keyword_retriever, vector_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# 4. Conversation chain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    memory=memory,\n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6878b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Nhân viên nào trong công ty đã tốt nghiệp Đại học Ngoại thương?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
